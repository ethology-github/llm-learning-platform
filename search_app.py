import streamlit as st
import os
import json
import re
from typing import List, Dict, Optional
from datetime import datetime

# å®šä¹‰è¯¾ç¨‹ç›®å½•ç»“æž„ (ä¸Ž learning_app.py ä¿æŒä¸€è‡´)
COURSE_STRUCTURE = {
    "C1 å¤§åž‹è¯­è¨€æ¨¡åž‹ LLM ä»‹ç»": [
        "1.å¤§åž‹è¯­è¨€æ¨¡åž‹ LLM ç†è®ºç®€ä»‹.md",
        "2.æ£€ç´¢å¢žå¼ºç”Ÿæˆ RAG ç®€ä»‹.md",
        "3.LangChain ç®€ä»‹.md",
        "4.å¼€å‘ LLM åº”ç”¨çš„æ•´ä½“æµç¨‹.md",
        "5.é˜¿é‡Œäº‘æœåŠ¡å™¨çš„åŸºæœ¬ä½¿ç”¨.md",
        "6.GitHub Codespaces çš„åŸºæœ¬ä½¿ç”¨ï¼ˆé€‰ä¿®ï¼‰.md",
        "7.çŽ¯å¢ƒé…ç½®.md"
    ],
    "C2 ä½¿ç”¨ LLM API å¼€å‘åº”ç”¨": [
        "1. åŸºæœ¬æ¦‚å¿µ.md",
        "2. ä½¿ç”¨ LLM API.ipynb",
        "3. Prompt Engineering.ipynb"
    ],
    "C3 æ­å»ºçŸ¥è¯†åº“": [
        "1.è¯å‘é‡åŠå‘é‡çŸ¥è¯†åº“ä»‹ç».md",
        "2.ä½¿ç”¨ Embedding API.ipynb",
        "3.æ•°æ®å¤„ç†.ipynb",
        "4.æ­å»ºå¹¶ä½¿ç”¨å‘é‡æ•°æ®åº“.ipynb"
    ],
    "C4 æž„å»º RAG åº”ç”¨": [
        "1.LLM æŽ¥å…¥ LangChain.ipynb",
        "2.æž„å»ºæ£€ç´¢é—®ç­”é“¾.ipynb",
        "3.éƒ¨ç½²çŸ¥è¯†åº“åŠ©æ‰‹.ipynb"
    ],
    "C5 ç³»ç»Ÿè¯„ä¼°ä¸Žä¼˜åŒ–": [
        "1.å¦‚ä½•è¯„ä¼° LLM åº”ç”¨.ipynb",
        "2.è¯„ä¼°å¹¶ä¼˜åŒ–ç”Ÿæˆéƒ¨åˆ†.ipynb",
        "3.è¯„ä¼°å¹¶ä¼˜åŒ–æ£€ç´¢éƒ¨åˆ†.md"
    ]
}

# è¯»å–Markdownæ–‡ä»¶å†…å®¹
def read_markdown_file(file_path):
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        st.warning(f"æ— æ³•è¯»å–æ–‡ä»¶ {file_path}: {str(e)}")
        return None

# æœç´¢åŠŸèƒ½
def search_content(query: str) -> List[Dict]:
    if not query or len(query.strip()) < 2:
        return []
        
    results = []
    query_lower = query.lower().strip()
    
    for chapter_name, files in COURSE_STRUCTURE.items():
        for file_name in files:
            # åªæœç´¢ Markdown æ–‡ä»¶
            if not file_name.endswith(".md"):
                continue

            file_path = os.path.join(
                os.path.dirname(__file__),
                "notebook",
                chapter_name,
                file_name
            )
            
            content = read_markdown_file(file_path)
            if content and not content.startswith("æ— æ³•è¯»å–æ–‡ä»¶") and not content.startswith("æ–‡ä»¶ä¸å­˜åœ¨"):
                content_lower = content.lower()
                
                # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…é¡¹
                matches = list(re.finditer(rf"{re.escape(query_lower)}", content_lower, re.IGNORECASE))
                
                if matches:
                    # ä¸ºæ¯ä¸ªæ–‡ä»¶åªä¿ç•™æœ€åŒ¹é…çš„å‡ ä¸ªç»“æžœ
                    file_results = []
                    for i, match in enumerate(matches[:3]):  # æœ€å¤š3ä¸ªåŒ¹é…
                        match_pos = match.start()
                        
                        # æå–æ›´å¥½çš„åŒ¹é…ç‰‡æ®µï¼ˆæŒ‰å¥å­åˆ†å‰²ï¼‰
                        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', content)
                        best_sentence = ""
                        best_sentence_index = -1
                        
                        for j, sentence in enumerate(sentences):
                            if query_lower in sentence.lower():
                                best_sentence = sentence.strip()
                                best_sentence_index = j
                                break
                        
                        if best_sentence:
                            snippet = best_sentence
                            # èŽ·å–ä¸Šä¸‹æ–‡
                            context_start = max(0, best_sentence_index - 1)
                            context_end = min(len(sentences), best_sentence_index + 2)
                            context = "ã€‚".join(sentences[context_start:context_end]).strip()
                            
                            # é«˜äº®åŒ¹é…è¯
                            highlighted_snippet = re.sub(
                                rf"({re.escape(query)})", 
                                r"**\1**", 
                                context, 
                                flags=re.IGNORECASE
                            )
                        else:
                            # å›žé€€åˆ°åŽŸå§‹æ–¹æ³•
                            start_idx = max(0, match_pos - 100)
                            end_idx = min(len(content), match_pos + len(query) + 100)
                            snippet = content[start_idx:end_idx]
                            highlighted_snippet = re.sub(
                                rf"({re.escape(query)})", 
                                r"**\1**", 
                                snippet, 
                                flags=re.IGNORECASE
                            )
                        
                        file_results.append({
                            "chapter": chapter_name,
                            "file": file_name,
                            "snippet": highlighted_snippet,
                            "path": file_path,
                            "match_count": len(matches)
                        })
                    
                    results.extend(file_results)
    
    return results

def main():
    st.title("ðŸ” å†…å®¹æœç´¢")
    st.write("åœ¨è¯¾ç¨‹å†…å®¹ä¸­æœç´¢æ‚¨æ„Ÿå…´è¶£çš„æŠ€æœ¯ç»†èŠ‚å’ŒçŸ¥è¯†ç‚¹ã€‚")
    
    # æœç´¢åŽ†å²
    if 'search_history' not in st.session_state:
        st.session_state['search_history'] = []
    
    # æœç´¢ç•Œé¢
    col1, col2 = st.columns([3, 1])
    with col1:
        search_query = st.text_input("è¾“å…¥æœç´¢å…³é”®è¯", key="search_input", placeholder="ä¾‹å¦‚ï¼šRAGã€APIã€å‘é‡æ•°æ®åº“...")
    with col2:
        st.write("")
        st.write("")
        search_button = st.button("ðŸ” æœç´¢", type="primary")
    
    # æ˜¾ç¤ºæœç´¢åŽ†å²
    if st.session_state['search_history']:
        with st.expander("ðŸ• æœç´¢åŽ†å²"):
            history_cols = st.columns(min(5, len(st.session_state['search_history'])))
            for i, term in enumerate(st.session_state['search_history']):
                with history_cols[i % 5]:
                    if st.button(term, key=f"history_{i}"):
                        st.session_state['search_input'] = term
                        st.rerun()
    
    # æ‰§è¡Œæœç´¢
    if (search_query and search_query.strip()) or search_button:
        query = search_query.strip()
        
        if len(query) < 2:
            st.warning("è¯·è¾“å…¥è‡³å°‘2ä¸ªå­—ç¬¦è¿›è¡Œæœç´¢")
        else:
            # æ·»åŠ åˆ°æœç´¢åŽ†å²
            if query not in st.session_state['search_history']:
                st.session_state['search_history'].insert(0, query)
                st.session_state['search_history'] = st.session_state['search_history'][:10]  # ä¿ç•™æœ€è¿‘10æ¬¡
            
            with st.spinner(f"æ­£åœ¨æœç´¢ '{query}'..."):
                results = search_content(query)
            
            if results:
                st.success(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³ç»“æžœ")
                
                # æŒ‰ç« èŠ‚åˆ†ç»„æ˜¾ç¤ºç»“æžœ
                results_by_chapter = {}
                for result in results:
                    chapter = result['chapter']
                    if chapter not in results_by_chapter:
                        results_by_chapter[chapter] = []
                    results_by_chapter[chapter].append(result)
                
                for chapter, chapter_results in results_by_chapter.items():
                    with st.expander(f"ðŸ“š {chapter} ({len(chapter_results)} ä¸ªç»“æžœ)", expanded=True):
                        for i, result in enumerate(chapter_results):
                            col_left, col_right = st.columns([4, 1])
                            
                            with col_left:
                                st.markdown(f"**ðŸ“„ {result['file']}**")
                                st.markdown(f"ðŸ“ {result['snippet']}")
                                
                                if 'match_count' in result:
                                    st.caption(f"å…± {result['match_count']} å¤„åŒ¹é…")
                            
                            with col_right:
                                st.write("")
                                if st.button("å‰å¾€å­¦ä¹ ", key=f"go_to_learning_{chapter}_{i}"):
                                    st.session_state['page'] = 'å¼€å§‹å­¦ä¹ '
                                    st.session_state['initial_chapter'] = result['chapter']
                                    st.session_state['initial_file'] = result['file']
                                    st.rerun()
                            
                            if i < len(chapter_results) - 1:
                                st.divider()
                
            else:
                st.info("ðŸ˜” æ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ç»“æžœ")
                st.markdown("**å»ºè®®ï¼š**")
                st.markdown("- å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯")
                st.markdown("- æ£€æŸ¥æ‹¼å†™æ˜¯å¦æ­£ç¡®")
                st.markdown("- ä½¿ç”¨æ›´é€šç”¨çš„è¯æ±‡")
                
                # æŽ¨èç›¸å…³æœç´¢
                suggested_terms = ["RAG", "API", "LangChain", "å‘é‡æ•°æ®åº“", "å¤§æ¨¡åž‹", "æç¤ºå·¥ç¨‹"]
                st.markdown("**è¯•è¯•è¿™äº›æœç´¢è¯ï¼š**")
                cols = st.columns(3)
                for i, term in enumerate(suggested_terms[:6]):
                    with cols[i % 3]:
                        if st.button(term, key=f"suggest_{i}"):
                            st.session_state['search_input'] = term
                            st.rerun()
    
    # æœç´¢æç¤º
    with st.expander("ðŸ’¡ æœç´¢æç¤º"):
        st.markdown("""
        - **å…³é”®è¯æœç´¢**: è¾“å…¥æ‚¨æ„Ÿå…´è¶£çš„æŠ€æœ¯æœ¯è¯­æˆ–æ¦‚å¿µ
        - **ä¸­è‹±æ–‡éƒ½æ”¯æŒ**: å¯ä»¥æœç´¢ä¸­æ–‡æˆ–è‹±æ–‡å…³é”®è¯
        - **æ¨¡ç³ŠåŒ¹é…**: ç³»ç»Ÿä¼šè‡ªåŠ¨æŸ¥æ‰¾ç›¸å…³å†…å®¹
        - **ç»“æžœæŽ’åº**: æŒ‰ç« èŠ‚åˆ†ç»„æ˜¾ç¤ºç›¸å…³ç»“æžœ
        - **å¿«é€Ÿè·³è½¬**: ç‚¹å‡»"å‰å¾€å­¦ä¹ "æŒ‰é’®ç›´æŽ¥è·³è½¬åˆ°å¯¹åº”å†…å®¹
        
        **æŽ¨èæœç´¢è¯**ï¼š
        - RAGã€æ£€ç´¢å¢žå¼ºç”Ÿæˆ
        - APIã€OpenAIã€æ™ºè°±AI
        - LangChainã€é“¾å¼è°ƒç”¨
        - å‘é‡æ•°æ®åº“ã€Chroma
        - æç¤ºå·¥ç¨‹ã€Prompt
        - å¤§æ¨¡åž‹ã€LLM
        """)

if __name__ == "__main__":
    main()